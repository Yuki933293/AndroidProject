import time
import subprocess
from dataclasses import dataclass
import io
import wave
import os

import numpy as np
import torch
import speech_recognition as sr
import dashscope
import requests  # ç”¨äºè°ƒç”¨å¿ƒçŸ¥å¤©æ°” API
import pyaudio
import struct
from openwakeword.model import Model as OWWModel
from openwakeword.model import Model as OWWModel

try:
    from dashscope.version import __version__ as dashscope_version
except ImportError:
    dashscope_version = None

from dashscope import Application
from dashscope.audio.asr.recognition import Recognition, RecognitionCallback
from http import HTTPStatus
import uuid

# ================= é…ç½®åŒºåŸŸ =================
# æ‰“å°å½“å‰ dashscope SDK ç‰ˆæœ¬ï¼Œä¾¿äºå®šä½å…¼å®¹æ€§é—®é¢˜
DASHSCOPE_SDK_VERSION = getattr(
    dashscope, '__version__', dashscope_version or 'unknown'
)
print(f"[INFO] dashscope SDK version: {DASHSCOPE_SDK_VERSION}")

# 1. è®¾ç½®é˜¿é‡Œäº‘ç™¾ç‚¼ API KEYï¼ˆè¿™é‡Œç”¨å ä½ç¬¦ï¼Œè®°å¾—æ¢æˆä½ è‡ªå·±çš„ï¼‰
dashscope.api_key = 'sk-fb64515c017945fc9282f9ace355cad3'

# 2. è®¾ç½®ä½ çš„åº”ç”¨ ID
APP_ID = '16356830643247938dfa31f8414fd58d'

# 3. å¿ƒçŸ¥å¤©æ°”é…ç½®ï¼ˆç›´æ¥å†™åœ¨ä»£ç é‡Œï¼›æ³¨æ„ä¸è¦æ³„éœ²åˆ°å…¬å¼€ä»“åº“ï¼‰
#    key ç›´æ¥ç”¨ç§é’¥æ–¹å¼ï¼š...now.json?key=your_private_key&location=beijing&language=zh-Hans&unit=c
SENIVERSE_KEY = "Sr1izeSrxPYouIE7B"  # TODO: æ›¿æ¢æˆä½ è‡ªå·±çš„å¿ƒçŸ¥å¤©æ°”ç§é’¥
SENIVERSE_WEATHER_NOW_URL = "https://api.seniverse.com/v3/weather/now.json"
# ===========================================
# openWakeWord å”¤é†’è¯é…ç½®
OWW_MODEL_PATHS = ["wakewords/xiaoguanjia.tflite"]  # è‡ªå®šä¹‰å”¤é†’è¯æ¨¡å‹è·¯å¾„
OWW_THRESHOLD = 0.5  # è§¦å‘é˜ˆå€¼ï¼Œå¯æŒ‰éœ€è¦è°ƒæ•´
# ä¼šè¯ IDï¼ˆå¤šè½®è®°å¿†ï¼‰
SESSION_ID = str(uuid.uuid4())

# é‡‡æ ·ç‡è®¾ç½®ï¼šå…¨éƒ¨ç»Ÿä¸€åˆ° 16k
MIC_SAMPLE_RATE = 16000
# VAD å®¿é†‰å‚æ•°
HANGOVER_GAP_SEC = 0.6   # è¯­éŸ³ç‰‡æ®µé—´å°äºè¯¥é—´éš”åˆ™åˆå¹¶
HANGOVER_PAD_SEC = 0.4   # å‰åå„æ‰©å±•çš„ç¼“å†²

# åƒé—® ASR æ¨¡å‹é…ç½®ï¼š16k ç‰ˆæœ¬
QWEN_ASR_MODEL = 'paraformer-realtime-v2'
QWEN_ASR_SAMPLE_RATE = 16000  # ä¸æ¨¡å‹ä¿æŒä¸€è‡´
WAKE_WORDS = ["å°ç®¡å®¶", "åŠ©æ‰‹", "ä½ å¥½", "å—¨"]  # ç®€æ˜“å”¤é†’è¯åˆ—è¡¨
oww_model: OWWModel | None = None

# ---------- Silero VAD åˆå§‹åŒ– ----------
print("[INFO] æ­£åœ¨åŠ è½½ Silero VAD æ¨¡å‹...")
silero_model, silero_utils = torch.hub.load(
    repo_or_dir='snakers4/silero-vad',
    model='silero_vad',
    force_reload=False
)
(get_speech_timestamps,
 _,
 _,
 _,
 _) = silero_utils
print("[INFO] Silero VAD æ¨¡å‹åŠ è½½å®Œæˆã€‚")

# æ’­æ”¾çŠ¶æ€æ ‡è®°ï¼ˆåŠåŒå·¥ï¼‰
is_playing: bool = False

# ---------- ASR å›è°ƒå ä½ ----------
class SimpleASRCallback(RecognitionCallback):
    def on_open(self): pass
    def on_complete(self): pass
    def on_error(self, result): pass
    def on_close(self): pass
    def on_event(self, result): pass


# ---------- ä½¿ç”¨ Silero VAD è£å‰ªè¯­éŸ³ ----------
def apply_silero_vad_to_audio(audio: sr.AudioData) -> sr.AudioData:
    """
    è¾“å…¥ï¼šspeech_recognition çš„ AudioDataï¼ˆå†…éƒ¨è½¬ä¸º 16kï¼‰
    è¿‡ç¨‹ï¼š
      1. è½¬ä¸º 16k å•å£°é“ wav bytes
      2. æå– PCM æ•°æ®ï¼Œè½¬ä¸º torch.Tensor
      3. ç”¨ Silero VAD è·å–è¯­éŸ³åŒºé—´
      4. æˆªå– [ç¬¬ä¸€ä¸ª start, æœ€åä¸€ä¸ª end] ä¹‹é—´çš„è¯­éŸ³
      5. è¿”å›æ–°çš„ AudioDataï¼ˆ16k, 16bitï¼‰
    """
    # 1. ä» AudioData è·å– 16k wav bytes
    wav_bytes = audio.get_wav_data(
        convert_rate=MIC_SAMPLE_RATE,
        convert_width=2  # 16-bit
    )

    # 2. ç”¨ wave æ¨¡å—è§£æ PCM
    with wave.open(io.BytesIO(wav_bytes), 'rb') as wf:
        num_channels = wf.getnchannels()
        assert num_channels == 1, f"æœŸæœ›å•å£°é“éŸ³é¢‘ï¼Œå®é™…é€šé“æ•°={num_channels}"
        sample_width = wf.getsampwidth()
        assert sample_width == 2, f"æœŸæœ› 16bit éŸ³é¢‘ï¼Œå®é™…ä½å®½={sample_width}"
        sample_rate = wf.getframerate()
        assert sample_rate == MIC_SAMPLE_RATE, f"é‡‡æ ·ç‡ä¸åŒ¹é…: {sample_rate}"
        num_frames = wf.getnframes()
        pcm_bytes = wf.readframes(num_frames)

    # 3. PCM bytes -> torch.Tensor (float32, -1~1)
    pcm16 = np.frombuffer(pcm_bytes, dtype=np.int16)
    if pcm16.size == 0:
        print("[VAD] PCM æ•°æ®ä¸ºç©º")
        return audio

    audio_tensor = torch.from_numpy(pcm16).float() / 32768.0

    # 4. Silero VAD è·å–è¯­éŸ³æ—¶é—´æˆ³
    speech_ts = get_speech_timestamps(
        audio_tensor,
        silero_model,
        sampling_rate=MIC_SAMPLE_RATE
    )

    if not speech_ts:
        print("[VAD] Silero æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¿”å›åŸå§‹ AudioDataã€‚")
        return audio

    # Hangover æœºåˆ¶ï¼šåˆå¹¶é—´éš”å¾ˆçŸ­çš„è¯­éŸ³æ®µï¼Œå¹¶åœ¨é¦–å°¾åŠ ç¼“å†²
    hangover_gap = int(HANGOVER_GAP_SEC * MIC_SAMPLE_RATE)
    pad = int(HANGOVER_PAD_SEC * MIC_SAMPLE_RATE)

    merged = []
    cur_start, cur_end = speech_ts[0]['start'], speech_ts[0]['end']
    for seg in speech_ts[1:]:
        if seg['start'] - cur_end <= hangover_gap:
            cur_end = max(cur_end, seg['end'])
        else:
            merged.append({'start': cur_start, 'end': cur_end})
            cur_start, cur_end = seg['start'], seg['end']
    merged.append({'start': cur_start, 'end': cur_end})

    start = max(0, merged[0]['start'] - pad)
    end = min(len(audio_tensor), merged[-1]['end'] + pad)
    print(f"[VAD] åˆå¹¶åè¯­éŸ³ç‰‡æ®µ: start={start}, end={end}, æ€»é•¿åº¦={len(audio_tensor)}")

    voiced = audio_tensor[start:end]

    # 5. å†è½¬å› int16 PCM bytes
    voiced = torch.clamp(voiced, -1.0, 1.0)
    voiced_int16 = (voiced * 32768.0).short().numpy().tobytes()

    # 6. å°è£…ä¸ºæ–°çš„ AudioDataï¼ˆ16k, 16bitï¼‰
    trimmed_audio = sr.AudioData(
        voiced_int16,
        MIC_SAMPLE_RATE,
        2  # sample width
    )
    return trimmed_audio


# ---------- ä½¿ç”¨åƒé—® ASR è¯†åˆ«æœ¬åœ°éŸ³é¢‘ ----------
def qwen_asr_transcribe(audio_bytes: bytes,
                        sample_rate: int = QWEN_ASR_SAMPLE_RATE) -> str | None:
    temp_path = 'temp_input.wav'
    try:
        with open(temp_path, 'wb') as f:
            f.write(audio_bytes)

        asr = Recognition(
            model=QWEN_ASR_MODEL,
            callback=SimpleASRCallback(),
            format='wav',
            sample_rate=sample_rate
        )
        result = asr.call(temp_path)
        sentence = result.get_sentence()
        print(f"[DEBUG] ASR status={result.status_code}, "
              f"code={result.code}, msg={result.message}, raw_sentence={sentence}")

        text = None
        if isinstance(sentence, list):
            # å–æœ€åä¸€å¥æœ‰æ•ˆæ–‡æœ¬
            for item in reversed(sentence):
                if isinstance(item, dict):
                    text = item.get('text') or item.get('result')
                    if text:
                        break
        elif isinstance(sentence, dict):
            text = sentence.get('text') or sentence.get('result')

        if text:
            return text

        print(f"[DEBUG] ASR è¿”å›æ— æ³•è§£æ: {sentence}")
        return None
    except Exception as e:
        print(f"[DEBUG] åƒé—® ASR è°ƒç”¨å¼‚å¸¸: {e}")
        return None


# ---------- å¿ƒçŸ¥å¤©æ°”ï¼šå®å†µå¤©æ°”å·¥å…· ----------
def get_weather_from_seniverse(location: str) -> str:
    """
    è°ƒç”¨å¿ƒçŸ¥å¤©æ°”å®å†µå¤©æ°”æ¥å£ï¼Œè¿”å›ä¸€æ®µé¢å‘ç”¨æˆ·çš„ä¸­æ–‡æè¿°ã€‚
    æ–‡æ¡£ç¤ºä¾‹ï¼š
      https://api.seniverse.com/v3/weather/now.json?key=your_api_key&location=beijing&language=zh-Hans&unit=c
    """
    if not SENIVERSE_KEY:
        return "å¤©æ°”æŸ¥è¯¢åŠŸèƒ½å°šæœªé…ç½®å¿ƒçŸ¥å¤©æ°”å¯†é’¥ã€‚"

    try:
        params = {
            "key": SENIVERSE_KEY,   # ç§é’¥
            "location": location,   # å¯ä»¥æ˜¯åŸå¸‚åã€æ‹¼éŸ³ã€ç»çº¬åº¦ç­‰
            "language": "zh-Hans",
            "unit": "c"
        }
        resp = requests.get(SENIVERSE_WEATHER_NOW_URL, params=params, timeout=5)
        resp.raise_for_status()
        data = resp.json()

        # è‹¥è¿”å›é”™è¯¯ç»“æ„ï¼š{"status": "...", "status_code": "AP010010"}
        if not isinstance(data, dict):
            return f"æŸ¥è¯¢ {location} çš„å¤©æ°”æ—¶è¿”å›äº†å¼‚å¸¸æ•°æ®ã€‚"

        if "results" not in data:
            status = data.get("status")
            status_code = data.get("status_code")
            if status or status_code:
                print(f"[DEBUG] Seniverse error: status={status}, code={status_code}")
                return f"å¿ƒçŸ¥å¤©æ°”æ¥å£è¿”å›é”™è¯¯ï¼š{status or 'æœªçŸ¥é”™è¯¯'}ï¼ˆ{status_code or 'æ— é”™è¯¯ç '}ï¼‰ã€‚"
            return f"æŸ¥è¯¢ {location} çš„å¤©æ°”å¤±è´¥ï¼Œæœªè·å¾—æœ‰æ•ˆç»“æœã€‚"

        results = data.get("results") or []
        if not results:
            return f"æ²¡æœ‰æ‰¾åˆ° {location} çš„å¤©æ°”ä¿¡æ¯ã€‚"

        first = results[0]
        loc = first.get("location", {}) or {}
        now = first.get("now", {}) or {}
        last_update = first.get("last_update")

        # location ä¿¡æ¯ï¼Œå°½é‡åªä¿ç•™ä¸€ä¸ªåŸå¸‚åï¼Œé¿å…é‡å¤
        path = loc.get("path")
        name = loc.get("name")
        place = None
        if path:
            # path ç¤ºä¾‹ï¼š"è¥¿é›…å›¾,åç››é¡¿å·,ç¾å›½"ï¼Œä»…å–ç¬¬ä¸€æ®µåŸå¸‚
            first = str(path).split(",")[0].strip()
            place = first or None
        if not place and name:
            place = str(name).strip()
        if not place:
            place = location

        text = now.get("text")            # å¤©æ°”ç°è±¡æ–‡å­—ï¼Œä¾‹å¦‚â€œå¤šäº‘â€
        temperature = now.get("temperature")
        humidity = now.get("humidity")
        wind_speed = now.get("wind_speed")
        wind_direction = now.get("wind_direction")

        parts = []
        if place:
            parts.append(f"{place} å½“å‰å¤©æ°”ï¼š{text or 'æš‚æ— '}")
        else:
            parts.append(f"{location} å½“å‰å¤©æ°”ï¼š{text or 'æš‚æ— '}")

        if temperature is not None:
            parts.append(f"æ°”æ¸© {temperature}â„ƒ")
        if humidity is not None:
            parts.append(f"æ¹¿åº¦ {humidity}%")
        if wind_direction or wind_speed is not None:
            # é£å‘/é£é€Ÿæœ‰å“ªä¸ªè¯´å“ªä¸ª
            wind_desc = []
            if wind_direction:
                wind_desc.append(wind_direction)
            if wind_speed is not None:
                wind_desc.append(f"{wind_speed} å…¬é‡Œ/å°æ—¶")
            if wind_desc:
                parts.append("é£å‘é£é€Ÿï¼š" + " ".join(wind_desc))
        if last_update:
            parts.append(f"ï¼ˆæ•°æ®æ›´æ–°æ—¶é—´ï¼š{last_update}ï¼‰")

        return "ï¼Œ".join(parts) + "ã€‚"

    except Exception as e:
        print(f"[DEBUG] Seniverse è¯·æ±‚å¼‚å¸¸: {e}")
        return f"æŸ¥è¯¢ {location} çš„å¤©æ°”æ—¶å‡ºé”™äº†ï¼Œè¯·ç¨åå†è¯•ã€‚"


def extract_city_from_text(text: str) -> str | None:
    """
    ç®€å•è§„åˆ™è§£æâ€œåŸå¸‚ + å¤©æ°”â€ï¼š
      - å¦‚æœå¥å­é‡ŒåŒ…å«â€œå¤©æ°”â€ä¸¤å­—ï¼Œåˆ™å°è¯•æŠŠ â€œå¤©æ°”â€ å‰åçš„æ–‡å­—å½“ä½œåœ°åã€‚
      - ä¾‹ï¼š
        â€œä¸Šæµ·å¤©æ°”æ€ä¹ˆæ ·â€          -> åŸå¸‚ï¼šä¸Šæµ·
        â€œå¸®æˆ‘æŸ¥ä¸€ä¸‹åŒ—äº¬çš„å¤©æ°”â€    -> åŸå¸‚ï¼šåŒ—äº¬
        â€œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·â€          -> è§£æä¸å‡ºåŸå¸‚ï¼Œè¿”å› None
    """
    if "å¤©æ°”" not in text:
        return None

    text = text.strip()
    idx = text.find("å¤©æ°”")

    # æƒ…å½¢1ï¼š"...åŸå¸‚...å¤©æ°”..."
    if idx > 0:
        before = text[:idx]
        for ch in ["çš„", "ç°åœ¨", "ä»Šæ—¥", "ä»Šå¤©", "ä»Šå„¿", "ä¸€ä¸‹", "æŸ¥æŸ¥", "å¸®æˆ‘", "å¸®æˆ‘æŸ¥", "å¸®å¿™æŸ¥"]:
            before = before.replace(ch, "")
        city = before.strip()
        if city:
            return city

    # æƒ…å½¢2ï¼š"å¤©æ°”...åŸå¸‚..."ï¼ˆè¾ƒå°‘è§ï¼‰
    after = text[idx + len("å¤©æ°”"):]
    for ch in ["çš„", "ç°åœ¨", "ä»Šæ—¥", "ä»Šå¤©", "ä»Šå„¿", "ä¸€ä¸‹", "æŸ¥æŸ¥", "å¸®æˆ‘", "å¸®æˆ‘æŸ¥", "å¸®å¿™æŸ¥"]:
        after = after.replace(ch, "")
    after = after.strip()
    if after:
        return after

    return None
# ---------- /å¿ƒçŸ¥å¤©æ°”ï¼šå®å†µå¤©æ°”å·¥å…· ----------


# åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å™¨
r = sr.Recognizer()


def speak(text, t_llm_last: float | None = None):
    print(f"ğŸ¤– æ™ºèƒ½ä½“æ­£åœ¨ç”Ÿæˆè¯­éŸ³: {text} ...")
    global is_playing
    is_playing = True
    try:
        # ä½¿ç”¨ macOS å†…ç½® say å‘½ä»¤è¿›è¡Œ TTS æ’­æ”¾
        subprocess.run(["say", text], check=False)
    finally:
        is_playing = False


def listen():
    # åŠåŒå·¥ï¼šè‹¥æœºå™¨äººåœ¨æ’­æŠ¥ï¼Œå…ˆç­‰å¾…æ’­æŠ¥ç»“æŸ
    while is_playing:
        time.sleep(0.05)

    with sr.Microphone(sample_rate=MIC_SAMPLE_RATE) as source:
        print("\nğŸ‘‚ æ­£åœ¨è†å¬... (è¯·è¯´è¯)")
        # è‡ªåŠ¨è°ƒæ•´ç¯å¢ƒå™ªéŸ³ï¼Œæ­¤æ—¶æé†’ç”¨æˆ·å…ˆä¸è¦è¯´è¯
        r.adjust_for_ambient_noise(source, duration=1.0)
        # è°ƒæ•´é™éŸ³åˆ¤å®šï¼Œé¿å…è¿‡æ—©æˆªæ–­
        r.pause_threshold = 1.2
        r.non_speaking_duration = 0.3
        r.energy_threshold = max(r.energy_threshold, 150)

        try:
            print("å¼€å§‹å½•éŸ³ï¼ˆspeech_recognition + Silero VAD è£å‰ªï¼‰...")
            # ä½¿ç”¨ speech_recognition çš„ç›‘å¬é€»è¾‘
            audio = r.listen(
                source,
                timeout=12,            # æœ€å¤šç­‰ç”¨æˆ· 12 ç§’å¼€å§‹è¯´è¯
                phrase_time_limit=30   # å•æ¬¡æœ€é•¿å½•éŸ³ 30 ç§’
            )

            # ç”¨ Silero VAD è¿›ä¸€æ­¥è£å‰ªå¤´å°¾é™éŸ³
            trimmed_audio = apply_silero_vad_to_audio(audio)

            # ç›´æ¥ä»¥ 16k è¾“å‡º wav bytesï¼Œäº¤ç»™ 16k ASR
            wav_data = trimmed_audio.get_wav_data(
                convert_rate=QWEN_ASR_SAMPLE_RATE,
                convert_width=2
            )

            text = qwen_asr_transcribe(wav_data)
            if text:
                print(f"ğŸ‘¤ æ‚¨è¯´: {text}")
            else:
                print("...ASR æœªè¯†åˆ«å‡ºæœ‰æ•ˆæ–‡æœ¬")
            return text

        except sr.WaitTimeoutError:
            print("...è¶…æ—¶æœªæ£€æµ‹åˆ°è¯­éŸ³")
            return None
        except sr.UnknownValueError:
            print("...æ— æ³•ç†è§£éŸ³é¢‘")
            return None


def listen_wakeup() -> bool:
    """ä½¿ç”¨ openWakeWord è¿›è¡Œå”¤é†’è¯æ£€æµ‹ï¼Œæ£€æµ‹åˆ°è¿”å› True"""
    global oww_model
    if oww_model is None:
        try:
            # æ–°ç‰ˆæœ¬å‚æ•°ä¸º wakeword_models
            model_paths = [p for p in OWW_MODEL_PATHS if p and os.path.exists(p)]
            if not model_paths:
                print(f"[WAKE] æœªæ‰¾åˆ°å”¤é†’è¯æ¨¡å‹æ–‡ä»¶ï¼š{OWW_MODEL_PATHS}")
                return False
            oww_model = OWWModel(wakeword_models=model_paths)
            print(f"[WAKE] openWakeWord æ¨¡å‹åŠ è½½å®Œæˆ: {model_paths}")
        except Exception as e:
            print(f"[WAKE] openWakeWord åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    sample_rate = getattr(oww_model, "sample_rate", MIC_SAMPLE_RATE)
    frame_length = getattr(oww_model, "window_samples", 512)

    pa = pyaudio.PyAudio()
    stream = pa.open(rate=sample_rate,
                     channels=1,
                     format=pyaudio.paInt16,
                     input=True,
                     frames_per_buffer=frame_length)

    print("[WAKE] æ­£åœ¨ç›‘å¬å”¤é†’è¯ï¼ˆopenWakeWordï¼‰...")
    triggered = False
    end_time = time.time() + 15  # æœ€å¤šç›‘å¬ 15 ç§’
    try:
        while time.time() < end_time:
            pcm = stream.read(frame_length, exception_on_overflow=False)
            audio_int16 = np.frombuffer(pcm, dtype=np.int16)
            preds = oww_model.predict(audio_int16)
            if preds and any(score >= OWW_THRESHOLD for score in preds.values()):
                print(f"[WAKE] æ£€æµ‹åˆ°å”¤é†’è¯ï¼ˆopenWakeWordï¼‰ï¼š{preds}")
                triggered = True
                break
    except Exception as e:
        print(f"[WAKE] openWakeWord ç›‘å¬å¼‚å¸¸: {e}")
    finally:
        stream.stop_stream()
        stream.close()
        pa.terminate()

    return triggered


def chat_with_agent(prompt):
    """è°ƒç”¨ç™¾ç‚¼æ™ºèƒ½ä½“"""
    try:
        response = Application.call(
            app_id=APP_ID,
            prompt=prompt,
            session_id=SESSION_ID  # ä¼ å…¥å›ºå®š session_idï¼Œå®ç°å¤šè½®è®°å¿†
        )

        if response.status_code != HTTPStatus.OK:
            print(f'âŒ è¯·æ±‚å¤±è´¥: {response.message}')
            return "å¯¹ä¸èµ·ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹å¤´æ™•ï¼Œè¯·ç¨åå†è¯•ã€‚"

        return response.output.text

    except Exception as e:
        print(f"âŒ è°ƒç”¨å¼‚å¸¸: {e}")
        return "ç³»ç»Ÿå‡ºé”™äº†ã€‚"


def main():
    print("ç­‰å¾…å”¤é†’è¯ä¸­...")

    while True:
        # å…ˆç­‰å¾…å”¤é†’è¯
        if not listen_wakeup():
            continue

        # å”¤é†’åé—®å€™ä¸€æ¬¡
        speak("ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„æ™ºèƒ½ç®¡å®¶ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„ï¼Ÿ")

        # è¿›å…¥å¯¹è¯å¾ªç¯
        while True:
            user_text = listen()

            if user_text:
                # é€€å‡ºæœºåˆ¶
                if "å†è§" in user_text or "é€€å‡º" in user_text:
                    speak("å¥½çš„ï¼Œå†è§ï¼")
                    break

                # ===== å¤©æ°”å·¥å…·è·¯ç”±é€»è¾‘ï¼ˆå¿ƒçŸ¥å¤©æ°”ï¼‰=====
                city = extract_city_from_text(user_text)
                if city:
                    print(f"[ROUTER] æ£€æµ‹åˆ°å¤©æ°”æŸ¥è¯¢ï¼ŒåŸå¸‚ = {city}")
                    reply_text = get_weather_from_seniverse(city)
                    llm_end_ts = time.time()
                else:
                    # éå¤©æ°”é—®é¢˜ï¼Œäº¤ç»™ç™¾ç‚¼æ™ºèƒ½ä½“
                    reply_text = chat_with_agent(user_text)
                    llm_end_ts = time.time()

                # 3. è¯´
                speak(reply_text, t_llm_last=llm_end_ts)

            # ç®€å•é˜²åˆ·å±ç­‰å¾…
            time.sleep(0.5)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nç¨‹åºå·²æ‰‹åŠ¨åœæ­¢")
