import time
import subprocess
from dataclasses import dataclass
import io
import wave

import numpy as np
import torch
import speech_recognition as sr
import dashscope
import requests  # ç”¨äºè°ƒç”¨ WeatherAPI

try:
    from dashscope.version import __version__ as dashscope_version
except ImportError:
    dashscope_version = None

from dashscope import Application
from dashscope.audio.asr.recognition import Recognition, RecognitionCallback
from http import HTTPStatus
import uuid

# ================= é…ç½®åŒºåŸŸ =================
# æ‰“å°å½“å‰ dashscope SDK ç‰ˆæœ¬ï¼Œä¾¿äºå®šä½å…¼å®¹æ€§é—®é¢˜
DASHSCOPE_SDK_VERSION = getattr(
    dashscope, '__version__', dashscope_version or 'unknown'
)
print(f"[INFO] dashscope SDK version: {DASHSCOPE_SDK_VERSION}")

# 1. è®¾ç½®é˜¿é‡Œäº‘ç™¾ç‚¼ API KEY
dashscope.api_key = 'sk-fb64515c017945fc9282f9ace355cad3'

# 2. è®¾ç½®ä½ çš„åº”ç”¨ ID
APP_ID = '16356830643247938dfa31f8414fd58d'

# 3. WeatherAPI é…ç½®ï¼ˆç›´æ¥å†™åœ¨ä»£ç é‡Œï¼›æ³¨æ„ä¸è¦æ³„éœ²åˆ°å…¬å¼€ä»“åº“ï¼‰
WEATHERAPI_KEY = "82987c29ea4e465ab65111554250912"  
WEATHERAPI_BASE_URL = "https://api.weatherapi.com/v1/current.json"
# ===========================================
# ä¼šè¯ IDï¼ˆå¤šè½®è®°å¿†ï¼‰
SESSION_ID = str(uuid.uuid4())

# é‡‡æ ·ç‡è®¾ç½®ï¼šå…¨éƒ¨ç»Ÿä¸€åˆ° 16k
MIC_SAMPLE_RATE = 16000
# VAD å®¿é†‰å‚æ•°
HANGOVER_GAP_SEC = 0.6   # è¯­éŸ³ç‰‡æ®µé—´å°äºè¯¥é—´éš”åˆ™åˆå¹¶
HANGOVER_PAD_SEC = 0.4   # å‰åå„æ‰©å±•çš„ç¼“å†²

# åƒé—® ASR æ¨¡å‹é…ç½®ï¼š16k ç‰ˆæœ¬
QWEN_ASR_MODEL = 'paraformer-realtime-v2'
QWEN_ASR_SAMPLE_RATE = 16000  # ä¸æ¨¡å‹ä¿æŒä¸€è‡´

# ---------- Silero VAD åˆå§‹åŒ– ----------
print("[INFO] æ­£åœ¨åŠ è½½ Silero VAD æ¨¡å‹...")
silero_model, silero_utils = torch.hub.load(
    repo_or_dir='snakers4/silero-vad',
    model='silero_vad',
    force_reload=False
)
(get_speech_timestamps,
 _,
 _,
 _,
 _) = silero_utils
print("[INFO] Silero VAD æ¨¡å‹åŠ è½½å®Œæˆã€‚")

# æ’­æ”¾çŠ¶æ€æ ‡è®°ï¼ˆåŠåŒå·¥ï¼‰
is_playing: bool = False

# ---------- ASR å›è°ƒå ä½ ----------
class SimpleASRCallback(RecognitionCallback):
    def on_open(self): pass
    def on_complete(self): pass
    def on_error(self, result): pass
    def on_close(self): pass
    def on_event(self, result): pass


# ---------- ä½¿ç”¨ Silero VAD è£å‰ªè¯­éŸ³ ----------
def apply_silero_vad_to_audio(audio: sr.AudioData) -> sr.AudioData:
    """
    è¾“å…¥ï¼šspeech_recognition çš„ AudioDataï¼ˆå†…éƒ¨è½¬ä¸º 16kï¼‰
    è¿‡ç¨‹ï¼š
      1. è½¬ä¸º 16k å•å£°é“ wav bytes
      2. æå– PCM æ•°æ®ï¼Œè½¬ä¸º torch.Tensor
      3. ç”¨ Silero VAD è·å–è¯­éŸ³åŒºé—´
      4. æˆªå– [ç¬¬ä¸€ä¸ª start, æœ€åä¸€ä¸ª end] ä¹‹é—´çš„è¯­éŸ³
      5. è¿”å›æ–°çš„ AudioDataï¼ˆ16k, 16bitï¼‰
    """
    # 1. ä» AudioData è·å– 16k wav bytes
    wav_bytes = audio.get_wav_data(
        convert_rate=MIC_SAMPLE_RATE,
        convert_width=2  # 16-bit
    )

    # 2. ç”¨ wave æ¨¡å—è§£æ PCM
    with wave.open(io.BytesIO(wav_bytes), 'rb') as wf:
        num_channels = wf.getnchannels()
        assert num_channels == 1, f"æœŸæœ›å•å£°é“éŸ³é¢‘ï¼Œå®é™…é€šé“æ•°={num_channels}"
        sample_width = wf.getsampwidth()
        assert sample_width == 2, f"æœŸæœ› 16bit éŸ³é¢‘ï¼Œå®é™…ä½å®½={sample_width}"
        sample_rate = wf.getframerate()
        assert sample_rate == MIC_SAMPLE_RATE, f"é‡‡æ ·ç‡ä¸åŒ¹é…: {sample_rate}"
        num_frames = wf.getnframes()
        pcm_bytes = wf.readframes(num_frames)

    # 3. PCM bytes -> torch.Tensor (float32, -1~1)
    pcm16 = np.frombuffer(pcm_bytes, dtype=np.int16)
    if pcm16.size == 0:
        print("[VAD] PCM æ•°æ®ä¸ºç©º")
        return audio

    audio_tensor = torch.from_numpy(pcm16).float() / 32768.0

    # 4. Silero VAD è·å–è¯­éŸ³æ—¶é—´æˆ³
    speech_ts = get_speech_timestamps(
        audio_tensor,
        silero_model,
        sampling_rate=MIC_SAMPLE_RATE
    )

    if not speech_ts:
        print("[VAD] Silero æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¿”å›åŸå§‹ AudioDataã€‚")
        return audio

    # Hangover æœºåˆ¶ï¼šåˆå¹¶é—´éš”å¾ˆçŸ­çš„è¯­éŸ³æ®µï¼Œå¹¶åœ¨é¦–å°¾åŠ ç¼“å†²
    hangover_gap = int(HANGOVER_GAP_SEC * MIC_SAMPLE_RATE)
    pad = int(HANGOVER_PAD_SEC * MIC_SAMPLE_RATE)

    merged = []
    cur_start, cur_end = speech_ts[0]['start'], speech_ts[0]['end']
    for seg in speech_ts[1:]:
        if seg['start'] - cur_end <= hangover_gap:
            cur_end = max(cur_end, seg['end'])
        else:
            merged.append({'start': cur_start, 'end': cur_end})
            cur_start, cur_end = seg['start'], seg['end']
    merged.append({'start': cur_start, 'end': cur_end})

    start = max(0, merged[0]['start'] - pad)
    end = min(len(audio_tensor), merged[-1]['end'] + pad)
    print(f"[VAD] åˆå¹¶åè¯­éŸ³ç‰‡æ®µ: start={start}, end={end}, æ€»é•¿åº¦={len(audio_tensor)}")

    voiced = audio_tensor[start:end]

    # 5. å†è½¬å› int16 PCM bytes
    voiced = torch.clamp(voiced, -1.0, 1.0)
    voiced_int16 = (voiced * 32768.0).short().numpy().tobytes()

    # 6. å°è£…ä¸ºæ–°çš„ AudioDataï¼ˆ16k, 16bitï¼‰
    trimmed_audio = sr.AudioData(
        voiced_int16,
        MIC_SAMPLE_RATE,
        2  # sample width
    )
    return trimmed_audio


# ---------- ä½¿ç”¨åƒé—® ASR è¯†åˆ«æœ¬åœ°éŸ³é¢‘ ----------
def qwen_asr_transcribe(audio_bytes: bytes,
                        sample_rate: int = QWEN_ASR_SAMPLE_RATE) -> str | None:
    temp_path = 'temp_input.wav'
    try:
        with open(temp_path, 'wb') as f:
            f.write(audio_bytes)

        asr = Recognition(
            model=QWEN_ASR_MODEL,
            callback=SimpleASRCallback(),
            format='wav',
            sample_rate=sample_rate
        )
        result = asr.call(temp_path)
        sentence = result.get_sentence()
        print(f"[DEBUG] ASR status={result.status_code}, "
              f"code={result.code}, msg={result.message}, raw_sentence={sentence}")

        text = None
        if isinstance(sentence, list):
            # å–æœ€åä¸€å¥æœ‰æ•ˆæ–‡æœ¬
            for item in reversed(sentence):
                if isinstance(item, dict):
                    text = item.get('text') or item.get('result')
                    if text:
                        break
        elif isinstance(sentence, dict):
            text = sentence.get('text') or sentence.get('result')

        if text:
            return text

        print(f"[DEBUG] ASR è¿”å›æ— æ³•è§£æ: {sentence}")
        return None
    except Exception as e:
        print(f"[DEBUG] åƒé—® ASR è°ƒç”¨å¼‚å¸¸: {e}")
        return None


# ---------- WeatherAPI å¤©æ°”å·¥å…· ----------
def get_weather_from_weatherapi(location: str) -> str:
    if not WEATHERAPI_KEY:
        return "å¤©æ°”æŸ¥è¯¢åŠŸèƒ½å°šæœªé…ç½® WeatherAPI å¯†é’¥ã€‚"

    try:
        params = {
            "key": WEATHERAPI_KEY,
            "q": location,   # å¯ä»¥æ˜¯åŸå¸‚åã€ç»çº¬åº¦ç­‰
            "aqi": "no"      # ä¸è¦ç©ºæ°”è´¨é‡ï¼Œå‡å°‘è¿”å›ä½“ç§¯
        }
        resp = requests.get(WEATHERAPI_BASE_URL, params=params, timeout=5)
        resp.raise_for_status()
        data = resp.json()

        loc = data.get("location", {}) or {}
        cur = data.get("current", {}) or {}

        city_name = loc.get("name", location)
        region = loc.get("region", "")
        country = loc.get("country", "")

        temp_c = cur.get("temp_c")
        feels_c = cur.get("feelslike_c")
        condition = (cur.get("condition") or {}).get("text", "")
        humidity = cur.get("humidity")
        wind_kph = cur.get("wind_kph")
        last_updated = cur.get("last_updated")

        # ç»„åˆåœ°ç‚¹ä¿¡æ¯å¹¶å»é‡ï¼Œé¿å…â€œä¸­å›½BeijingåŒ—äº¬â€é‡å¤
        # åªå–ä¸€ä¸ªåœ°ç‚¹ï¼Œä¼˜å…ˆåŸå¸‚ï¼Œå…¶æ¬¡åŒºåŸŸï¼Œå†æ¬¡å›½å®¶ï¼Œé¿å…é‡å¤ï¼ˆå¦‚â€œä¸­å›½ Beijing åŒ—äº¬â€ï¼‰
        if city_name and str(city_name).strip():
            place_str = str(city_name).strip()
        elif region and str(region).strip():
            place_str = str(region).strip()
        elif country and str(country).strip():
            place_str = str(country).strip()
        else:
            place_str = location

        parts = []
        if place_str:
            parts.append(f"{place_str}å½“å‰å¤©æ°”ï¼š{condition or 'æš‚æ— '}")
        else:
            parts.append(f"{location}å½“å‰å¤©æ°”ï¼š{condition or 'æš‚æ— '}")

        if temp_c is not None:
            parts.append(f"æ°”æ¸© {temp_c}â„ƒ")
        if feels_c is not None:
            parts.append(f"ä½“æ„Ÿ {feels_c}â„ƒ")
        if humidity is not None:
            parts.append(f"æ¹¿åº¦ {humidity}%")
        if wind_kph is not None:
            parts.append(f"é£é€Ÿ {wind_kph} å…¬é‡Œ/å°æ—¶")
        if last_updated:
            parts.append(f"ï¼ˆæ•°æ®æ›´æ–°æ—¶é—´ï¼š{last_updated}ï¼‰")

        return "ï¼Œ".join(parts) + "ã€‚"

    except Exception as e:
        print(f"[DEBUG] WeatherAPI è¯·æ±‚å¼‚å¸¸: {e}")
        return f"æŸ¥è¯¢ {location} çš„å¤©æ°”æ—¶å‡ºé”™äº†ï¼Œè¯·ç¨åå†è¯•ã€‚"

def extract_city_from_text(text: str) -> str | None:
    """
    éä¸¥æ ¼ NLUï¼Œä»…ç”¨äº Demoï¼š
    è§„åˆ™ï¼š
      - å¦‚æœå¥å­é‡ŒåŒ…å«â€œå¤©æ°”â€ä¸¤å­—ï¼Œåˆ™å°è¯•æŠŠ â€œå¤©æ°”â€ å‰åçš„æ–‡å­—å½“ä½œåœ°åã€‚
      - ä¾‹å¦‚ï¼š
        - â€œä¸Šæµ·å¤©æ°”æ€ä¹ˆæ ·â€ -> åŸå¸‚ï¼šä¸Šæµ·
        - â€œå¸®æˆ‘æŸ¥ä¸€ä¸‹åŒ—äº¬çš„å¤©æ°”â€ -> åŸå¸‚ï¼šåŒ—äº¬
        - â€œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·â€ -> è§£æä¸å‡ºåŸå¸‚ï¼Œè¿”å› None
    """
    if "å¤©æ°”" not in text:
        return None

    text = text.strip()
    idx = text.find("å¤©æ°”")

    # æƒ…å½¢1ï¼š"...åŸå¸‚...å¤©æ°”..."
    if idx > 0:
        before = text[:idx]
        for ch in ["çš„", "ç°åœ¨", "ä»Šæ—¥", "ä»Šå¤©", "ä»Šå„¿", "ä¸€ä¸‹", "æŸ¥æŸ¥"]:
            before = before.replace(ch, "")
        city = before.strip()
        if city:
            return city

    # æƒ…å½¢2ï¼š"å¤©æ°”...åŸå¸‚..."ï¼ˆè¾ƒå°‘è§ï¼‰
    after = text[idx + len("å¤©æ°”"):]
    for ch in ["çš„", "ç°åœ¨", "ä»Šæ—¥", "ä»Šå¤©", "ä»Šå„¿", "ä¸€ä¸‹", "æŸ¥æŸ¥"]:
        after = after.replace(ch, "")
    after = after.strip()
    if after:
        return after

    return None
# ---------- /WeatherAPI å¤©æ°”å·¥å…· ----------


# åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å™¨
r = sr.Recognizer()


def speak(text, t_llm_last: float | None = None):
    print(f"ğŸ¤– æ™ºèƒ½ä½“æ­£åœ¨ç”Ÿæˆè¯­éŸ³: {text} ...")
    global is_playing
    is_playing = True
    try:
        # ä½¿ç”¨ macOS å†…ç½® say å‘½ä»¤è¿›è¡Œ TTS æ’­æ”¾
        subprocess.run(["say", text], check=False)
    finally:
        is_playing = False


def listen():
    # åŠåŒå·¥ï¼šè‹¥æœºå™¨äººåœ¨æ’­æŠ¥ï¼Œå…ˆç­‰å¾…æ’­æŠ¥ç»“æŸ
    while is_playing:
        time.sleep(0.05)

    with sr.Microphone(sample_rate=MIC_SAMPLE_RATE) as source:
        print("\nğŸ‘‚ æ­£åœ¨è†å¬... (è¯·è¯´è¯)")
        # è‡ªåŠ¨è°ƒæ•´ç¯å¢ƒå™ªéŸ³ï¼Œæ­¤æ—¶æé†’ç”¨æˆ·å…ˆä¸è¦è¯´è¯
        r.adjust_for_ambient_noise(source, duration=1.0)
        # è°ƒæ•´é™éŸ³åˆ¤å®šï¼Œé¿å…è¿‡æ—©æˆªæ–­
        r.pause_threshold = 1.2
        r.non_speaking_duration = 0.3
        r.energy_threshold = max(r.energy_threshold, 150)

        try:
            print("å¼€å§‹å½•éŸ³ï¼ˆspeech_recognition + Silero VAD è£å‰ªï¼‰...")
            # ä½¿ç”¨ speech_recognition çš„ç›‘å¬é€»è¾‘
            audio = r.listen(
                source,
                timeout=12,            # æœ€å¤šç­‰ç”¨æˆ· 12 ç§’å¼€å§‹è¯´è¯
                phrase_time_limit=30   # å•æ¬¡æœ€é•¿å½•éŸ³ 30 ç§’
            )

            # ç”¨ Silero VAD è¿›ä¸€æ­¥è£å‰ªå¤´å°¾é™éŸ³
            trimmed_audio = apply_silero_vad_to_audio(audio)

            # ç›´æ¥ä»¥ 16k è¾“å‡º wav bytesï¼Œäº¤ç»™ 16k ASR
            wav_data = trimmed_audio.get_wav_data(
                convert_rate=QWEN_ASR_SAMPLE_RATE,
                convert_width=2
            )

            text = qwen_asr_transcribe(wav_data)
            if text:
                print(f"ğŸ‘¤ æ‚¨è¯´: {text}")
            else:
                print("...ASR æœªè¯†åˆ«å‡ºæœ‰æ•ˆæ–‡æœ¬")
            return text

        except sr.WaitTimeoutError:
            print("...è¶…æ—¶æœªæ£€æµ‹åˆ°è¯­éŸ³")
            return None
        except sr.UnknownValueError:
            print("...æ— æ³•ç†è§£éŸ³é¢‘")
            return None


def chat_with_agent(prompt):
    """è°ƒç”¨ç™¾ç‚¼æ™ºèƒ½ä½“"""
    try:
        response = Application.call(
            app_id=APP_ID,
            prompt=prompt,
            session_id=SESSION_ID  # ä¼ å…¥å›ºå®š session_idï¼Œå®ç°å¤šè½®è®°å¿†
        )

        if response.status_code != HTTPStatus.OK:
            print(f'âŒ è¯·æ±‚å¤±è´¥: {response.message}')
            return "å¯¹ä¸èµ·ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹å¤´æ™•ï¼Œè¯·ç¨åå†è¯•ã€‚"

        return response.output.text

    except Exception as e:
        print(f"âŒ è°ƒç”¨å¼‚å¸¸: {e}")
        return "ç³»ç»Ÿå‡ºé”™äº†ã€‚"


def main():
    speak("å°ç‘åœ¨çš„ï¼Ÿ")

    while True:
        # 1. å¬
        user_text = listen()

        if user_text:
            # é€€å‡ºæœºåˆ¶
            if "å†è§" in user_text or "é€€å‡º" in user_text:
                speak("å¥½çš„ï¼Œå†è§ï¼")
                break

            # ===== å¤©æ°”å·¥å…·è·¯ç”±é€»è¾‘ =====
            city = extract_city_from_text(user_text)
            if city:
                print(f"[ROUTER] æ£€æµ‹åˆ°å¤©æ°”æŸ¥è¯¢ï¼ŒåŸå¸‚ = {city}")
                reply_text = get_weather_from_weatherapi(city)
                llm_end_ts = time.time()
            else:
                # éå¤©æ°”é—®é¢˜ï¼Œäº¤ç»™ç™¾ç‚¼æ™ºèƒ½ä½“
                reply_text = chat_with_agent(user_text)
                llm_end_ts = time.time()

            # 3. è¯´
            speak(reply_text, t_llm_last=llm_end_ts)

        # ç®€å•é˜²åˆ·å±ç­‰å¾…
        time.sleep(0.5)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nç¨‹åºå·²æ‰‹åŠ¨åœæ­¢")
